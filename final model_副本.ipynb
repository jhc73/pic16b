{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52dec5b6-7c89-4f0a-8158-329d8295b3e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['video_id', 'trending_date', 'title', 'channel_title', 'category_id',\n",
      "       'publish_time', 'tags', 'views', 'likes', 'dislikes', 'comment_count',\n",
      "       'thumbnail_link', 'comments_disabled', 'ratings_disabled',\n",
      "       'video_error_or_removed', 'description'],\n",
      "      dtype='object')\n",
      "      video_id trending_date  \\\n",
      "0  2kyS6SvSYSE      17.14.11   \n",
      "1  1ZAPwfrtAFY      17.14.11   \n",
      "2  5qpjK5DgCt4      17.14.11   \n",
      "3  puqaWrEC7tY      17.14.11   \n",
      "4  d380meD0W0M      17.14.11   \n",
      "\n",
      "                                               title          channel_title  \\\n",
      "0                 WE WANT TO TALK ABOUT OUR MARRIAGE           CaseyNeistat   \n",
      "1  The Trump Presidency: Last Week Tonight with J...        LastWeekTonight   \n",
      "2  Racist Superman | Rudy Mancuso, King Bach & Le...           Rudy Mancuso   \n",
      "3                   Nickelback Lyrics: Real or Fake?  Good Mythical Morning   \n",
      "4                           I Dare You: GOING BALD!?               nigahiga   \n",
      "\n",
      "   category_id              publish_time  \\\n",
      "0           22  2017-11-13T17:13:01.000Z   \n",
      "1           24  2017-11-13T07:30:00.000Z   \n",
      "2           23  2017-11-12T19:05:24.000Z   \n",
      "3           24  2017-11-13T11:00:04.000Z   \n",
      "4           24  2017-11-12T18:01:41.000Z   \n",
      "\n",
      "                                                tags    views   likes  \\\n",
      "0                                    SHANtell martin   748374   57527   \n",
      "1  last week tonight trump presidency|\"last week ...  2418783   97185   \n",
      "2  racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...  3191434  146033   \n",
      "3  rhett and link|\"gmm\"|\"good mythical morning\"|\"...   343168   10172   \n",
      "4  ryan|\"higa\"|\"higatv\"|\"nigahiga\"|\"i dare you\"|\"...  2095731  132235   \n",
      "\n",
      "   dislikes  comment_count                                  thumbnail_link  \\\n",
      "0      2966          15954  https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg   \n",
      "1      6146          12703  https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg   \n",
      "2      5339           8181  https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg   \n",
      "3       666           2146  https://i.ytimg.com/vi/puqaWrEC7tY/default.jpg   \n",
      "4      1989          17518  https://i.ytimg.com/vi/d380meD0W0M/default.jpg   \n",
      "\n",
      "   comments_disabled  ratings_disabled  video_error_or_removed  \\\n",
      "0              False             False                   False   \n",
      "1              False             False                   False   \n",
      "2              False             False                   False   \n",
      "3              False             False                   False   \n",
      "4              False             False                   False   \n",
      "\n",
      "                                         description  \n",
      "0  SHANTELL'S CHANNEL - https://www.youtube.com/s...  \n",
      "1  One year after the presidential election, John...  \n",
      "2  WATCH MY PREVIOUS VIDEO â¶ \\n\\nSUBSCRIBE âº ...  \n",
      "3  Today we find out if Link is a Nickelback amat...  \n",
      "4  I know it's been a while since we did this sho...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('USvideos.csv', encoding='latin1')  \n",
    "print(df.columns)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0e99d10-a11b-4bd7-baf6-a7bef2316844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['video_id', 'trending_date', 'title', 'channel_title', 'category_id', 'publish_time', 'tags', 'views', 'likes', 'dislikes', 'comment_count', 'thumbnail_link', 'comments_disabled', 'ratings_disabled', 'video_error_or_removed', 'description']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fe90b7-45d1-4e45-b8bb-4310a3dcd16c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24db4fd-12f8-4aa2-b8be-6636f37419e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594d7319-c24f-4553-9c48-27fc1854c2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45bb367b-01c0-4f48-9483-2373340a8028",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['video_id', 'title', 'tags', 'views', 'likes', 'comment_count', 'thumbnail_link']]\n",
    "df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1e64bcf-406a-4e57-a797-1d35da238872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['is_viral'] = df['likes'] > 10000\n",
    "df['is_viral'] = (df['likes'] > 10000) & (df['views'] > 1_000_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cb0a016-2d81-4ad7-b4e8-7704ebe3b2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(df['likes'], bins=100, log=True)\n",
    "plt.title(\"Like count distribution\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(df['views'], bins=100, log=True)\n",
    "plt.title(\"View count distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5676d285-ff94-41ff-8f5a-62994a2b6942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7ef7ac6-4d8e-402b-9ff1-7c72c82d0506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|█████████████████████████████████████████| 100/100 [00:01<00:00, 68.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80        15\n",
      "           1       0.40      0.40      0.40         5\n",
      "\n",
      "    accuracy                           0.70        20\n",
      "   macro avg       0.60      0.60      0.60        20\n",
      "weighted avg       0.70      0.70      0.70        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Just Kaggle small dataset \n",
    "# =============================\n",
    "# def of（viral） \n",
    "df['is_viral'] = (df['likes'] > 10000) & (df['views'] > 1_000_000)\n",
    "\n",
    "# =============================\n",
    "# the tumbnail column in csv(pic)\n",
    "# =============================\n",
    "import os\n",
    "import requests\n",
    "\n",
    "os.makedirs('thumbnails', exist_ok=True)\n",
    "\n",
    "def download_thumbnail(row):\n",
    "    video_id = row['video_id']\n",
    "    url = row['thumbnail_link']\n",
    "    path = f'thumbnails/{video_id}.jpg'\n",
    "    if not os.path.exists(path):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=10)\n",
    "            with open(path, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "            print(f'Downloaded: {video_id}')\n",
    "        except Exception as e:\n",
    "            print(f'Failed: {video_id} - {e}')\n",
    "\n",
    "df.head(100).apply(download_thumbnail, axis=1)  # \n",
    "\n",
    "# =============================\n",
    "# get vec\n",
    "# =============================\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "resnet.fc = torch.nn.Identity()  #\n",
    "resnet = resnet.to(device)\n",
    "resnet.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def extract_image_feature(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            features = resnet(img_tensor)\n",
    "        return features.squeeze().cpu().numpy()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "features = []\n",
    "for vid in tqdm(df['video_id'].head(100)):\n",
    "    path = f'thumbnails/{vid}.jpg'\n",
    "    if os.path.exists(path):\n",
    "        vec = extract_image_feature(path)\n",
    "    else:\n",
    "        vec = None\n",
    "    features.append(vec)\n",
    "\n",
    "df = df.head(100).copy()\n",
    "df['image_feature'] = features\n",
    "df = df[df['image_feature'].notnull()]  # del null\n",
    "\n",
    "# =============================\n",
    "# stand\n",
    "# =============================\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_num = df[['views', 'likes', 'comment_count']]\n",
    "scaler = StandardScaler()\n",
    "X_num_scaled = scaler.fit_transform(X_num)\n",
    "\n",
    "X_img = np.vstack(df['image_feature'].values)\n",
    "X = np.hstack([X_img, X_num_scaled])\n",
    "y = df['is_viral'].astype(int).values\n",
    "\n",
    "# =============================\n",
    "# log reg\n",
    "# =============================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb8d7fd-7ef6-4e34-b91b-120cc0ea3e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0a62458-480c-4a77-a014-b5acd62d560e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.2017\n",
      "Epoch 2, Loss: 3.4034\n",
      "Epoch 3, Loss: 3.1553\n",
      "Epoch 4, Loss: 3.0369\n",
      "Epoch 5, Loss: 3.2273\n",
      "Epoch 6, Loss: 2.6965\n",
      "Epoch 7, Loss: 2.4100\n",
      "Epoch 8, Loss: 2.0311\n",
      "Epoch 9, Loss: 2.9327\n",
      "Epoch 10, Loss: 2.3928\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93        77\n",
      "           1       1.00      0.52      0.69        23\n",
      "\n",
      "    accuracy                           0.89       100\n",
      "   macro avg       0.94      0.76      0.81       100\n",
      "weighted avg       0.90      0.89      0.88       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class MixedMLP(nn.Module):\n",
    "    def __init__(self, input_dim=515):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()  # use sigmoid for this one\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# just the pipeline\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# loss opt \n",
    "model = MixedMLP(input_dim=X.shape[1]).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "#train\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(X_tensor.to(device)).cpu().numpy()\n",
    "    pred_class = (pred > 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y, pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d3068de-cf62-472b-af15-7d1a1b287a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['video_id', 'title', 'tags', 'views', 'likes', 'comment_count', 'thumbnail_link', 'is_viral', 'image_feature']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29167b2-049b-4206-a5e8-702321d0229e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27a2b3ec-4547-49ac-b9a6-0ccd29b715b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text, thumbnail, length, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32803fa4-2952-419f-b48d-3565ea36f7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5927\n",
      "Epoch 2, Loss: 0.5086\n",
      "Epoch 3, Loss: 0.5195\n",
      "Epoch 4, Loss: 0.4678\n",
      "Epoch 5, Loss: 0.4338\n",
      "Epoch 6, Loss: 0.3935\n",
      "Epoch 7, Loss: 0.4140\n",
      "Epoch 8, Loss: 0.3416\n",
      "Epoch 9, Loss: 0.2734\n",
      "Epoch 10, Loss: 0.3209\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.96        77\n",
      "           1       0.90      0.78      0.84        23\n",
      "\n",
      "    accuracy                           0.93       100\n",
      "   macro avg       0.92      0.88      0.90       100\n",
      "weighted avg       0.93      0.93      0.93       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "X_img = np.vstack(df['image_feature'].values)  \n",
    "\n",
    "\n",
    "titles = df['title'].tolist()\n",
    "tfidf = TfidfVectorizer(max_features=300, min_df=2, max_df=0.8)\n",
    "X_text = tfidf.fit_transform(titles).toarray()   # shape=(n_samples,300)\n",
    "\n",
    "\n",
    "X = np.hstack([X_img, X_text])                  # shape=(n_samples, 812)\n",
    "y = df['is_viral'].astype(int).values           # shape=(n_samples,)\n",
    "\n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.reshape(-1,1), dtype=torch.float32)\n",
    "dataset  = TensorDataset(X_tensor, y_tensor)\n",
    "loader   = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "class MixedMLP(nn.Module):\n",
    "    def __init__(self, input_dim=812):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model     = MixedMLP(input_dim=X.shape[1]).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        loss   = criterion(model(xb), yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_prob  = model(X_tensor.to(device)).cpu().numpy()\n",
    "    pred_class = (pred_prob > 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y, pred_class))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54454f74-0c62-4340-a427-70a357f83041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "876b6658-1110-4169-af5b-0352717c68f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10  Train Loss: 0.5705\n",
      "Epoch 2/10  Train Loss: 0.4902\n",
      "Epoch 3/10  Train Loss: 0.4858\n",
      "Epoch 4/10  Train Loss: 0.4921\n",
      "Epoch 5/10  Train Loss: 0.4373\n",
      "Epoch 6/10  Train Loss: 0.4272\n",
      "Epoch 7/10  Train Loss: 0.4234\n",
      "Epoch 8/10  Train Loss: 0.3659\n",
      "Epoch 9/10  Train Loss: 0.3387\n",
      "Epoch 10/10  Train Loss: 0.2873\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7500    1.0000    0.8571        15\n",
      "         1.0     0.0000    0.0000    0.0000         5\n",
      "\n",
      "    accuracy                         0.7500        20\n",
      "   macro avg     0.3750    0.5000    0.4286        20\n",
      "weighted avg     0.5625    0.7500    0.6429        20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "#    df['image_feature']  (n_samples,512)\n",
    "#    df['title']         \n",
    "#    df['is_viral']       0/1\n",
    "\n",
    "# —— 2. title feature\n",
    "titles = df['title'].tolist()\n",
    "tfidf = TfidfVectorizer(max_features=300, min_df=2, max_df=0.8)\n",
    "X_text = tfidf.fit_transform(titles).toarray()  # (n,300)\n",
    "\n",
    "# —— 3. image\n",
    "X_img = np.vstack(df['image_feature'].values)   # (n,512)\n",
    "\n",
    "# —— 4. vec\n",
    "X_all = np.hstack([X_img, X_text])              # (n,812)\n",
    "y_all = df['is_viral'].astype(int).values       # (n,)\n",
    "\n",
    "# —— 5.tt split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_all\n",
    ")\n",
    "\n",
    "# —— 6. PyTorch TensorDataset & DataLoader\n",
    "def make_loader(X, y, batch_size=16, shuffle=True):\n",
    "    X_t = torch.tensor(X, dtype=torch.float32)\n",
    "    y_t = torch.tensor(y.reshape(-1,1), dtype=torch.float32)\n",
    "    ds  = TensorDataset(X_t, y_t)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "train_loader = make_loader(X_train, y_train, batch_size=32, shuffle=True)\n",
    "test_loader  = make_loader(X_test,  y_test,  batch_size=32, shuffle=False)\n",
    "\n",
    "# —— 7.  MixedMLP\n",
    "class MixedMLP(nn.Module):\n",
    "    def __init__(self, input_dim=812):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model     = MixedMLP(input_dim=X_all.shape[1]).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# —— 8. traun\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pred   = model(xb)\n",
    "        loss   = criterion(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}/{epochs}  Train Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# —— 9. valid\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels= []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        preds = model(xb).cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(yb.numpy())\n",
    "\n",
    "all_preds = np.vstack(all_preds)\n",
    "all_labels= np.vstack(all_labels)\n",
    "\n",
    "pred_classes = (all_preds > 0.5).astype(int)\n",
    "\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(all_labels, pred_classes, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90e5e3e-2d84-4bd7-a4a8-8d4c1a4b6bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcdd8abc-546c-45f9-83c2-b226672b864b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading thumbs: 100%|█████████████████| 40949/40949 [07:05<00:00, 96.18it/s]\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Extracting image features: 100%|██████████| 37390/37390 [09:12<00:00, 67.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10  Train Loss: 0.8071\n",
      "Epoch 2/10  Train Loss: 0.7147\n",
      "Epoch 3/10  Train Loss: 0.6233\n",
      "Epoch 4/10  Train Loss: 0.5561\n",
      "Epoch 5/10  Train Loss: 0.5113\n",
      "Epoch 6/10  Train Loss: 0.4809\n",
      "Epoch 7/10  Train Loss: 0.4529\n",
      "Epoch 8/10  Train Loss: 0.4213\n",
      "Epoch 9/10  Train Loss: 0.4198\n",
      "Epoch 10/10  Train Loss: 0.3860\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9539    0.8756    0.9131      4566\n",
      "         1.0     0.8272    0.9337    0.8772      2912\n",
      "\n",
      "    accuracy                         0.8982      7478\n",
      "   macro avg     0.8906    0.9047    0.8952      7478\n",
      "weighted avg     0.9046    0.8982    0.8991      7478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from torchvision import models, transforms\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ===========================\n",
    "# 1.read\n",
    "df = pd.read_csv('USvideos.csv', encoding='latin1')\n",
    "\n",
    "#  viral standard \n",
    "df['is_viral'] = ((df['views'] > 1_000_000) & (df['likes'] > 10_000)).astype(int)\n",
    "\n",
    "# \n",
    "# 2. image\n",
    "\n",
    "os.makedirs('thumbnails', exist_ok=True)\n",
    "\n",
    "def download_thumb(video_id, url):\n",
    "    path = f\"thumbnails/{video_id}.jpg\"\n",
    "    if not os.path.exists(path):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=5)\n",
    "            r.raise_for_status()\n",
    "            with open(path, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return path\n",
    "\n",
    "#\n",
    "thumb_paths = []\n",
    "for vid, url in tqdm(zip(df['video_id'], df['thumbnail_link']), \n",
    "                     total=len(df), desc=\"Downloading thumbs\"):\n",
    "    thumb_paths.append(download_thumb(vid, url))\n",
    "df['thumbnail_path'] = thumb_paths\n",
    "df = df[df['thumbnail_path'].notna()].reset_index(drop=True)\n",
    "\n",
    "# \n",
    "# 3. image\n",
    "\n",
    "#I use mac book\n",
    "#for the big one use my roommate's cuda\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "resnet.fc = nn.Identity()      # \n",
    "resnet.to(device).eval()\n",
    "\n",
    "img_tf = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],\n",
    "                         [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "def extract_img_feat(path):\n",
    "    try:\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        t   = img_tf(img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            feat = resnet(t)\n",
    "        return feat.squeeze().cpu().numpy()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "img_feats = []\n",
    "for p in tqdm(df['thumbnail_path'], desc=\"Extracting image features\"):\n",
    "    img_feats.append(extract_img_feat(p))\n",
    "df['img_feat'] = img_feats\n",
    "df = df[df['img_feat'].notnull()].reset_index(drop=True)\n",
    "\n",
    "# \n",
    "# 4. text\n",
    "# \n",
    "titles = df['title'].tolist()\n",
    "tfidf = TfidfVectorizer(max_features=300, min_df=5, max_df=0.8)\n",
    "X_text = tfidf.fit_transform(titles).toarray()  # (n,300)\n",
    "\n",
    "\n",
    "# 5.vec\n",
    "\n",
    "X_img = np.vstack(df['img_feat'].values)       # (n,512)\n",
    "X_all = np.hstack([X_img, X_text])              # (n,812)\n",
    "y_all = df['is_viral'].values                   # (n,)\n",
    "\n",
    "# 6. ==tt split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X_all, y_all,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_all\n",
    ")\n",
    "\n",
    "\n",
    "# 7. PyTorch Loader \n",
    "\n",
    "class_counts = np.bincount(y_tr)\n",
    "weights_cls  = 1.0 / class_counts\n",
    "sample_w     = weights_cls[y_tr]\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_w,\n",
    "    num_samples=len(sample_w),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "def make_loader(X, y, sampler=None):\n",
    "    Xt = torch.tensor(X, dtype=torch.float32)\n",
    "    yt = torch.tensor(y.reshape(-1,1), dtype=torch.float32)\n",
    "    ds = TensorDataset(Xt, yt)\n",
    "    return DataLoader(ds,\n",
    "                      batch_size=32,\n",
    "                      sampler=sampler,\n",
    "                      shuffle=(sampler is None))\n",
    "\n",
    "train_loader = make_loader(X_tr, y_tr, sampler=sampler)\n",
    "test_loader  = make_loader(X_te, y_te, sampler=None)\n",
    "\n",
    "# ===========================\n",
    "# 8. MixedMLP\n",
    "# ===========================\n",
    "class MixedMLP(nn.Module):\n",
    "    def __init__(self, input_dim=812):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)    # logit\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model     = MixedMLP(input_dim=X_all.shape[1]).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(\n",
    "    class_counts[0]/class_counts[1], device=device))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# ===========================\n",
    "# 9. train\n",
    "# ===========================\n",
    "epochs = 10\n",
    "for ep in range(1, epochs+1):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss   = criterion(logits, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running += loss.item()\n",
    "    print(f\"Epoch {ep}/{epochs}  Train Loss: {running/len(train_loader):.4f}\")\n",
    "\n",
    "# ===========================\n",
    "# 10.test\n",
    "\n",
    "# ===========================\n",
    "model.eval()\n",
    "all_probs, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "        probs  = torch.sigmoid(logits).cpu().numpy()\n",
    "        all_probs.append(probs)\n",
    "        all_labels.append(yb.numpy())\n",
    "\n",
    "all_probs  = np.vstack(all_probs)\n",
    "all_labels = np.vstack(all_labels)\n",
    "preds      = (all_probs > 0.5).astype(int)\n",
    "\n",
    "print(classification_report(all_labels, preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b0f6afd-d1b3-48a9-a025-ebfb02b5813a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 812])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ab7429c-a586-40d7-84e2-5fd6f0524e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['thumbnail_link'].iloc[0]\n",
    "#df['title'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7fb6e4a-21d5-412c-92d9-386ac6220a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7987e-01, 6.6763e-01, 7.2772e-01, 1.3825e-01, 2.5842e-01, 3.5179e-01,\n",
       "        4.1417e-01, 3.9617e-01, 2.3648e-01, 2.4252e-01, 3.5560e-01, 3.3000e-01,\n",
       "        1.2872e+00, 2.0027e-01, 1.0485e-01, 5.1922e-01, 2.0923e+00, 5.2484e-01,\n",
       "        2.7217e-01, 1.0182e-01, 5.4563e-01, 1.1566e-01, 2.5465e-01, 2.3196e+00,\n",
       "        2.2537e-01, 6.2917e-03, 9.1561e-01, 1.1778e+00, 1.5968e-01, 1.5925e+00,\n",
       "        2.1804e+00, 1.7272e-01, 4.8881e-01, 6.3441e-01, 1.3476e+00, 8.5540e-01,\n",
       "        1.9800e+00, 7.8465e-01, 1.3257e+00, 2.6526e-01, 2.5009e-01, 2.4257e-01,\n",
       "        4.5426e-02, 9.7351e-01, 1.2134e+00, 4.8228e-02, 9.6650e-01, 7.5331e-01,\n",
       "        5.8663e-01, 5.7101e-01, 6.2128e-01, 1.9089e-01, 1.0144e+00, 2.0348e-01,\n",
       "        1.0002e+00, 6.1561e-01, 3.7146e-01, 3.5425e-01, 8.8533e-01, 1.1362e+00,\n",
       "        1.0667e-02, 2.5280e-02, 9.8593e-01, 1.1415e+00, 4.2982e-02, 4.1580e-01,\n",
       "        5.6801e-01, 1.5082e-01, 7.3605e-01, 6.4164e-01, 5.2019e-01, 8.7903e-01,\n",
       "        1.1732e-01, 5.1254e-01, 3.1671e-03, 2.0899e-01, 1.9956e-02, 1.0853e+00,\n",
       "        1.5493e-01, 2.2686e-01, 3.3689e-01, 2.4738e+00, 1.9820e+00, 1.1981e+00,\n",
       "        1.1406e+00, 1.9764e-01, 8.8047e-02, 1.3702e-01, 7.9819e-01, 3.1922e+00,\n",
       "        3.8715e-01, 3.2623e-02, 1.5847e+00, 7.2552e-03, 3.9246e-01, 1.8640e-01,\n",
       "        2.9112e+00, 9.4974e-01, 0.0000e+00, 3.2746e-03, 1.0125e-01, 3.6700e-01,\n",
       "        1.0686e+00, 7.2847e-02, 2.2444e+00, 1.5646e+00, 3.5761e-01, 5.3999e-02,\n",
       "        7.5635e-01, 1.3191e+00, 4.9034e+00, 3.0519e-01, 5.2329e-01, 1.9938e+00,\n",
       "        1.3315e+00, 3.1069e+00, 1.4984e+00, 3.1873e+00, 8.3364e-01, 1.0504e+00,\n",
       "        1.1850e+00, 8.9401e-01, 2.3154e-01, 1.5971e-01, 3.5657e-01, 1.7564e+00,\n",
       "        1.4908e-01, 8.5261e-01, 1.1243e+00, 5.4380e-01, 8.3603e-01, 1.4553e-02,\n",
       "        4.9884e-01, 7.9873e-01, 4.9358e-01, 1.0692e+00, 2.8581e-02, 1.4490e+00,\n",
       "        1.4461e-02, 0.0000e+00, 7.6599e-01, 1.5658e-01, 8.9029e-01, 1.2086e+00,\n",
       "        5.2645e-01, 3.4531e-01, 4.9377e-01, 2.5865e+00, 4.6090e-01, 1.0585e-01,\n",
       "        7.7962e-03, 1.5152e-01, 2.2241e+00, 5.1425e-01, 5.1834e-01, 8.0600e-01,\n",
       "        2.9867e-01, 1.3460e-01, 0.0000e+00, 1.3309e+00, 5.0530e+00, 6.4093e-01,\n",
       "        1.0582e+00, 6.6187e+00, 1.5036e-02, 2.9756e-01, 7.3462e-04, 4.9744e-02,\n",
       "        4.2006e-02, 7.2521e-01, 5.1167e-02, 1.2080e-01, 9.2197e-01, 2.5165e-01,\n",
       "        7.3205e-04, 2.1117e+00, 1.0229e-01, 7.2837e-01, 3.1671e-02, 2.4859e-02,\n",
       "        3.2406e-01, 6.2986e-01, 6.2569e-01, 5.0389e-01, 1.4244e+00, 2.0845e-02,\n",
       "        1.9584e-01, 1.0616e-01, 3.8625e-01, 3.1085e-01, 1.1712e+00, 7.5060e-01,\n",
       "        2.2549e+00, 6.5752e-01, 1.0210e-01, 4.0636e-01, 1.2163e-01, 7.7568e-01,\n",
       "        7.9996e-01, 1.3332e+00, 5.1425e-01, 3.2614e-01, 3.6026e-01, 8.7570e-01,\n",
       "        3.0337e-02, 8.2426e-01, 3.0721e-01, 3.4651e-01, 1.5625e-02, 1.7508e-02,\n",
       "        6.5564e-02, 2.6811e-01, 5.8238e-02, 3.6012e-02, 6.0774e-01, 2.8376e-01,\n",
       "        8.6450e-01, 3.1429e-01, 8.9749e-02, 1.4840e+00, 3.7864e-02, 5.2367e-02,\n",
       "        2.5394e-01, 1.6299e-01, 1.8314e-01, 1.6347e-01, 2.0518e+00, 9.2574e-02,\n",
       "        4.4862e-02, 4.5737e-01, 1.7397e+00, 0.0000e+00, 2.1603e+00, 3.9986e-01,\n",
       "        5.2182e-02, 5.9037e-01, 7.6937e-01, 5.2160e-02, 9.4226e-01, 1.2305e+00,\n",
       "        6.2662e-01, 1.0638e+00, 1.1518e-01, 4.0089e-01, 4.2649e-01, 8.4889e-01,\n",
       "        3.0102e-01, 1.9880e-01, 1.7768e-01, 6.0048e-02, 7.9172e-01, 8.1112e-02,\n",
       "        6.0282e-02, 3.1543e-01, 1.2543e+00, 8.3296e-01, 4.7376e-01, 1.6835e-01,\n",
       "        2.5245e+00, 9.2166e-02, 7.0197e-02, 8.4526e-01, 7.3507e-01, 1.1139e-01,\n",
       "        9.9473e-02, 3.1213e-01, 1.6669e-01, 3.5856e-01, 2.2234e-01, 5.9406e-01,\n",
       "        5.8060e-01, 4.0119e-02, 2.6242e+00, 3.0227e+00, 5.0719e-01, 8.9220e-01,\n",
       "        9.1342e-01, 1.2038e+00, 1.3215e+00, 1.5205e-01, 7.9556e-02, 1.4030e-01,\n",
       "        1.4345e+00, 9.9342e-02, 3.1031e-01, 4.3629e-01, 8.0905e-03, 9.4673e-01,\n",
       "        4.5527e-01, 9.4373e-01, 1.7951e+00, 3.4729e-01, 1.1368e-01, 4.4804e-01,\n",
       "        1.7060e-01, 4.7621e-01, 2.4436e-01, 2.9247e-01, 1.1798e-01, 9.4605e-01,\n",
       "        1.4867e+00, 0.0000e+00, 7.8876e-01, 4.9840e-01, 1.2429e+00, 7.6520e-02,\n",
       "        3.6833e-02, 6.8697e-01, 3.7565e-01, 2.9427e-01, 2.1624e-01, 1.3921e+00,\n",
       "        4.3420e-01, 6.4216e-01, 4.1235e-01, 1.8449e-01, 1.9478e-01, 1.9400e-02,\n",
       "        4.3221e-02, 4.8719e-01, 2.4188e-01, 1.1283e-01, 1.0160e-01, 2.1242e-01,\n",
       "        5.1582e-01, 8.0608e-01, 2.8729e-01, 1.7223e-01, 2.1947e-01, 4.5350e-01,\n",
       "        1.3681e-01, 3.6503e-01, 2.4142e-02, 0.0000e+00, 5.8472e-01, 2.2755e-01,\n",
       "        3.0335e-01, 2.4138e-01, 1.8678e+00, 6.4745e-02, 1.3084e+00, 8.7033e-01,\n",
       "        3.5404e-02, 9.4072e-01, 9.6149e-01, 0.0000e+00, 6.8562e-01, 9.8656e-02,\n",
       "        6.2279e-02, 4.2841e-01, 5.2931e-02, 3.6187e-01, 4.8045e-01, 1.5814e-01,\n",
       "        1.3737e-02, 2.6755e-01, 1.3573e-01, 1.1621e-02, 2.1915e-02, 1.7621e-01,\n",
       "        1.9301e-02, 1.0932e+00, 1.7220e-01, 7.9625e-01, 7.5766e-01, 3.1469e-01,\n",
       "        6.7270e-01, 3.2032e+00, 2.2163e-01, 9.5318e-02, 1.6315e-01, 1.2540e-01,\n",
       "        8.0265e-02, 3.8692e+00, 1.0332e+00, 3.7785e-01, 2.5539e-01, 4.8440e-01,\n",
       "        2.4172e+00, 1.8442e+00, 7.6439e-01, 4.8750e-01, 3.2801e-01, 5.1960e-01,\n",
       "        9.6825e-03, 1.0636e-01, 3.5450e-01, 8.7384e-01, 2.3624e-04, 2.7264e-01,\n",
       "        3.8037e-01, 3.0900e-03, 7.1441e-04, 1.3768e-01, 7.5404e-01, 1.3707e+00,\n",
       "        5.7778e-01, 5.8177e-01, 1.1167e-02, 5.0418e-01, 5.1243e-02, 2.7561e+00,\n",
       "        2.4805e-01, 5.0845e-01, 1.1494e+00, 6.2631e-01, 1.5492e+00, 1.5846e-01,\n",
       "        4.4014e-01, 2.3900e+00, 1.8537e-01, 2.4331e-01, 4.4689e-01, 6.6230e-02,\n",
       "        8.1430e-01, 5.0420e-01, 1.4079e-01, 2.2246e-01, 1.7619e-01, 4.7771e-02,\n",
       "        4.5209e-03, 1.7668e-01, 3.3006e-01, 1.1253e+00, 2.8137e+00, 9.8991e-01,\n",
       "        4.3579e-02, 7.6190e-01, 1.3035e-01, 4.1377e-01, 2.3862e+00, 4.1558e-01,\n",
       "        8.5785e-01, 1.8481e-01, 9.1816e-01, 5.0610e-01, 2.4496e-01, 7.5985e-01,\n",
       "        1.2352e-01, 2.2981e+00, 5.5463e-01, 4.0844e-02, 1.5706e-01, 1.3176e+00,\n",
       "        9.8527e-01, 6.9424e-01, 1.1143e+00, 1.0895e-01, 1.5486e+00, 8.3387e-01,\n",
       "        1.4544e-01, 2.3489e-01, 1.2661e+00, 5.3632e-01, 1.0585e+00, 6.4283e-01,\n",
       "        1.3697e+00, 6.9924e-01, 6.7935e-01, 4.8442e-01, 1.1337e-01, 1.2918e+00,\n",
       "        7.6386e-02, 7.8765e-01, 7.2062e-03, 1.3355e+00, 2.6857e-01, 8.2453e-03,\n",
       "        1.3876e-01, 0.0000e+00, 3.2011e-01, 4.5247e-01, 1.0895e-01, 1.4262e-01,\n",
       "        2.4119e+00, 4.0648e-01, 4.5829e-01, 1.5281e-01, 9.4966e-02, 2.4757e+00,\n",
       "        7.8884e-01, 1.0020e+00, 1.4107e+00, 8.7046e-01, 1.0266e+00, 6.4854e-02,\n",
       "        2.3394e-01, 5.5037e-01, 8.7833e-02, 3.9150e-01, 5.8560e-02, 6.9768e-01,\n",
       "        6.9127e-01, 4.3779e-01, 1.0308e+00, 1.1765e+00, 1.6061e-01, 2.4617e-02,\n",
       "        9.3719e-01, 3.8810e-02, 3.5843e-01, 4.1113e-01, 2.1278e-01, 5.9888e-01,\n",
       "        1.3221e-01, 6.0917e-02, 1.5332e-01, 9.6674e-01, 1.0743e+00, 6.0061e-01,\n",
       "        6.9248e-03, 1.7121e-01])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb[0][:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9c605c54-a9e0-4268-b96b-04f400e42358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4406f6f-65c2-44da-a22c-bea922b42ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_viral(image_path, title_str, model, tfidf, img_tf, resnet, device, threshold=0.5):\n",
    "    try:\n",
    "        # image\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        t = img_tf(img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            img_vec = resnet(t).squeeze().cpu().numpy()  # (512,)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Image processing failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # text\n",
    "        text_vec = tfidf.transform([title_str]).toarray().squeeze()  # (300,)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Text vectorization failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # \n",
    "    full_vec = np.hstack([img_vec, text_vec]).reshape(1, -1)  # (1, 812)\n",
    "    xt = torch.tensor(full_vec, dtype=torch.float32).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logit = model(xt)\n",
    "        prob = torch.sigmoid(logit).item()\n",
    "\n",
    "    pred = int(prob > threshold)\n",
    "    print(f\"[Prediction] Viral Probability: {prob:.4f} → {'VIRAL ✅' if pred else 'Not Viral ❌'}\")\n",
    "    return prob, pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a07820b0-e01c-408b-b8fd-c0e9b72b29ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Prediction] Viral Probability: 0.0559 → Not Viral ❌\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.05588740482926369, 0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_str = \"Mr Beast\"\n",
    "image_path = \"mr_beast.jpg\"\n",
    "\n",
    "predict_viral(image_path, title_str, model, tfidf, img_tf, resnet, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a41337a6-95ed-4953-9b11-3c854f7b30d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mr_beast.jpg',\n",
       " 'final model.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'USvideos.csv',\n",
       " 'thumbnails']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61509c81-93dd-4271-9c3c-22fa0ef3ef7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "12a97fa7-157f-4593-a102-bb2d8688cacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullModel(nn.Module):\n",
    "    def __init__(self, tfidf_dim=300):\n",
    "        super().__init__()\n",
    "        # image\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        resnet.fc = nn.Identity()\n",
    "        self.resnet = resnet\n",
    "\n",
    "        # text\n",
    "        self.text_fc = nn.Sequential(\n",
    "            nn.Linear(tfidf_dim, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # MLP\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 + 128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  #  logit\n",
    "        )\n",
    "\n",
    "    def forward(self, image_tensor, text_tensor):\n",
    "        \"\"\"\n",
    "        image_tensor: (B, 3, 224, 224)\n",
    "        text_tensor: (B, 300)\n",
    "        \"\"\"\n",
    "        img_feat = self.resnet(image_tensor)             # (B, 512)\n",
    "        text_feat = self.text_fc(text_tensor)            # (B, 128)\n",
    "        x = torch.cat([img_feat, text_feat], dim=1)      # (B, 640)\n",
    "        return self.classifier(x)                        # (B, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "090c1441-81d4-4252-b964-b97a591fdf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"USvideos.csv\", encoding=\"latin1\", nrows=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbc4452-f9bc-4efc-84bb-c47d91ad8277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4479a6cd-33de-47b5-b27f-2e53fa2fa394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 20000/20000 [01:01<00:00, 325.61it/s]\n",
      "Epoch 1:  32%|█████████▉                     | 319/1000 [05:20<11:25,  1.01s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 122\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img, txt, lbl \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    121\u001b[0m     img, txt, lbl \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(device), txt\u001b[38;5;241m.\u001b[39mto(device), lbl\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 122\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(img, txt)\n\u001b[1;32m    123\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(out, lbl)\n\u001b[1;32m    124\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[71], line 88\u001b[0m, in \u001b[0;36mFullModel.forward\u001b[0;34m(self, image_tensor, text_tensor)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_tensor, text_tensor):\n\u001b[0;32m---> 88\u001b[0m     img_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresnet(image_tensor)\n\u001b[1;32m     89\u001b[0m     text_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_fc(text_tensor)\n\u001b[1;32m     90\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([img_feat, text_feat], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_impl(x)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/resnet.py:275\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m--> 275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[1;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n\u001b[1;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/resnet.py:96\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m     94\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m---> 96\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)\n\u001b[1;32m     97\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[1;32m    551\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import models, transforms\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ========== 1. Load CSV ==========\n",
    "df = pd.read_csv('USvideos.csv', encoding='latin1')\n",
    "df = df.head(20000).copy()\n",
    "df['is_viral'] = ((df['views'] > 1_000_000) & (df['likes'] > 10_000)).astype(int)\n",
    "\n",
    "# ========== 2. Download Thumbnails ==========\n",
    "os.makedirs('thumbnails', exist_ok=True)\n",
    "def download_thumb(video_id, url):\n",
    "    path = f\"thumbnails/{video_id}.jpg\"\n",
    "    if not os.path.exists(path):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=5)\n",
    "            with open(path, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        except:\n",
    "            return None\n",
    "    return path\n",
    "\n",
    "df['thumbnail_path'] = [download_thumb(vid, url) for vid, url in tqdm(zip(df['video_id'], df['thumbnail_link']), total=len(df))]\n",
    "df = df[df['thumbnail_path'].notna()].reset_index(drop=True)\n",
    "\n",
    "# ========== 3. TF-IDF ==========\n",
    "tfidf = TfidfVectorizer(max_features=300, min_df=1, max_df=0.9)\n",
    "X_text = tfidf.fit_transform(df['title']).toarray()\n",
    "\n",
    "# ========== 4. Image Transform ==========\n",
    "img_tf = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ========== 5. Custom Dataset ==========\n",
    "class FullDataset(Dataset):\n",
    "    def __init__(self, df, X_text, transform):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.X_text = X_text\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Image\n",
    "        path = self.df.loc[idx, 'thumbnail_path']\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        # Text\n",
    "        text_feat = torch.tensor(self.X_text[idx], dtype=torch.float32)\n",
    "        # Label\n",
    "        label = torch.tensor(self.df.loc[idx, 'is_viral'], dtype=torch.float32).unsqueeze(0)\n",
    "        return image, text_feat, label\n",
    "\n",
    "# ========== 6. Model ==========\n",
    "class FullModel(nn.Module):\n",
    "    def __init__(self, tfidf_dim=300):\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        self.text_fc = nn.Sequential(\n",
    "            nn.Linear(tfidf_dim, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 + 128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, image_tensor, text_tensor):\n",
    "        img_feat = self.resnet(image_tensor)\n",
    "        text_feat = self.text_fc(text_tensor)\n",
    "        x = torch.cat([img_feat, text_feat], dim=1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ========== 7. Dataloaders ==========\n",
    "device = torch.device(\"cpu\")\n",
    "dataset = FullDataset(df, X_text, img_tf)\n",
    "y_all = df['is_viral'].values\n",
    "X_train_idx, X_test_idx = train_test_split(\n",
    "    np.arange(len(y_all)), test_size=0.2, stratify=y_all, random_state=42\n",
    ")\n",
    "train_ds = torch.utils.data.Subset(dataset, X_train_idx)\n",
    "test_ds  = torch.utils.data.Subset(dataset, X_test_idx)\n",
    "\n",
    "# Weighted sampler to balance\n",
    "class_counts = np.bincount(y_all[X_train_idx])\n",
    "weights_cls = 1.0 / class_counts\n",
    "sample_w = weights_cls[y_all[X_train_idx]]\n",
    "sampler = WeightedRandomSampler(weights=sample_w, num_samples=len(sample_w), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, sampler=sampler)\n",
    "test_loader = DataLoader(test_ds, batch_size=16, shuffle=False)\n",
    "\n",
    "# ========== 8. Train ==========\n",
    "model = FullModel(tfidf_dim=300).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(class_counts[0] / class_counts[1], device=device))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for img, txt, lbl in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "        img, txt, lbl = img.to(device), txt.to(device), lbl.to(device)\n",
    "        out = model(img, txt)\n",
    "        loss = criterion(out, lbl)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[Epoch {epoch}] Train Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# ========== 9. Evaluate ==========\n",
    "model.eval()\n",
    "all_probs, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for img, txt, lbl in test_loader:\n",
    "        img, txt = img.to(device), txt.to(device)\n",
    "        probs = torch.sigmoid(model(img, txt)).cpu().numpy()\n",
    "        all_probs.append(probs)\n",
    "        all_labels.append(lbl.numpy())\n",
    "\n",
    "probs = np.vstack(all_probs)\n",
    "labels = np.vstack(all_labels)\n",
    "preds = (probs > 0.5).astype(int)\n",
    "\n",
    "print(classification_report(labels, preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a1b8db-944f-43a8-8533-26d6e31923c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
